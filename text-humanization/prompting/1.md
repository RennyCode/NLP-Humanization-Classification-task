PROMPT:
-------
i want you think of determinstic rules to humanize LLM output

exercise
```
Part 1: Humanize LLM Output
The Problem
As we all know, as humans it's often easy to identify text generated by most language models, such as ChatGPT. These texts usually feel robotic, monotonic, generic, lifeless, and "×¡××—×™".
For our use case, the conversational agents (AKA chatbots) need to have a "human-like" vibe, particularly those who are tasked with engaging with users in a more friendly and "fun" way to establish a personal connection.
Note, this is different from what is called "guardrails", i.e., ensuring, semantically or lexically, that the model response is aligned with the pre-defined scope of topics (e.g., ensuring that we won't send an offensive message to the user or won't send something related to politics).
The Task
In this part, you are tasked with writing a function that humanizes the Language Model responses in a chatbot. The goal is to ensure that the user engaging with the bot will not notice that it is an AI.
Itâ€™s important that we always maintain the original semantics of the text, such that:
embedding(original_text) ~ embedding(humanized_text)
As long as we would use LLM to â€œhumanizeâ€ an LLM response, it will always resemble the output it was trained to give - so we want to go for a classic rule-based solution (not LLM based).
Some points that might help you along the way:
How can we deal with the subjectiveness resulting from the variance of how different people are accustomed to phrasing differently? In other words, text can be humanized in many different styles.
What are the features or characteristics that make text appear more AI-like or human-like? try to think of as many as you can
Can they be measured?
Can they be controlled (based on a scale)?
```

currently some brainstorming i did:
Capital letters
Emojis
Text Length
Remove markdown
Either remove commas / dots or make spaces (â€œhello ,â€ instead of â€œhello,â€)
â€œAs an AIâ€¦â€


make a list of 20 rules.
for each rule, write the purpose, 3 examples, and the psuedocode of the algorithm used. 
each rule should 'fix' the issue (process the text and fix it) and not just detect it.
i also want to measure numerically each rule (give each rule a score of 0 - 1) because i want each to have a "fix" function and a "score" function
----






Below is a list of 20 deterministic, ruleâ€based transformations (â€œrulesâ€) designed to â€œhumanizeâ€ LLM output. Each rule is described with its purpose, three concrete examples of its transformation, and pseudocode outlining both a fix (transformation) function and a score function (that returns a value between 0 and 1 based on how â€œAI-likeâ€ or â€œhuman-likeâ€ the text is along that dimension).

---

### 1. Capitalization Variation Rule
**Purpose:**  
Introduce slight, controlled deviations in standard capitalization to mimic natural human inconsistency (e.g., an occasional missed capital letter at the start of a sentence).

**Examples:**  
- **Example 1:**  
  Input: `"I went to the store."`  
  Output (if applying variation): `"i went to the store."`
- **Example 2:**  
  Input: `"It's a sunny day."`  
  Output: `"it's a sunny day."`
- **Example 3:**  
  Input: `"Hello, how are you?"`  
  Output: `"hello, how are you?"`

**Pseudocode:**
```pseudo
function fix_capitalization(text, variation_level):
    sentences = split_text_into_sentences(text)
    for sentence in sentences:
        // Randomly decide whether to lowercase the first letter of each sentence
        if random_chance(variation_level):
            sentence[0] = to_lowercase(sentence[0])
            
        // Add random capitalization changes to individual words
        words = split_sentence_into_words(sentence)
        for word in words:
            // Occasionally lowercase a capitalized word or capitalize a lowercase word
            if length(word) > 3 and random_chance(variation_level * 0.3):
                if is_capitalized(word[0]):
                    word[0] = to_lowercase(word[0])
                else if random_chance(0.2):  // Less chance of random capitalization
                    word[0] = to_uppercase(word[0])
    
    return join_sentences(sentences)

function score_capitalization(text):
    sentences = split_text_into_sentences(text)
    errors = count_sentences_with_lowercase_start(sentences)
    random_caps = count_words_with_random_capitalization(text)
    total_words = count_words(text)
    
    score = (errors / length(sentences) + random_caps / total_words) / 2
    return clamp(score, 0, 1)
```

---

### 2. Emoji Injection Rule
**Purpose:**  
Insert context-appropriate emojis to convey tone and emotion, adding a more personable, human feel.

**Examples:**  
- **Example 1:**  
  Input: `"I love this idea!"`  
  Output: `"I love this idea! ðŸ˜Š"`
- **Example 2:**  
  Input: `"That's interesting."`  
  Output: `"That's interesting ðŸ¤”."`
- **Example 3:**  
  Input: `"Great job."`  
  Output: `"Great job! ðŸ‘"`

**Pseudocode:**
```pseudo
function fix_emojis(text, injection_probability):
    sentences = split_text_into_sentences(text)
    emojis = ["ðŸ˜Š", "ðŸ¤”", "ðŸ‘", "ðŸ˜‚", "â¤ï¸", "ðŸ™Œ", "ðŸ˜Ž", "ðŸ¤·â€â™€ï¸", "ðŸ‘€", "ðŸ”¥"]
    
    for sentence in sentences:
        // Randomize whether to add emoji and where
        if random_chance(injection_probability):
            emoji = random_choice(emojis)
            
            // Randomly choose position - end of sentence, middle, or after specific word types
            position_type = random_choice(["end", "middle", "after_verb", "after_adjective"])
            
            if position_type == "end":
                sentence += " " + emoji
            else if position_type == "middle":
                insertion_point = random_int(0, length(sentence) - 1)
                sentence = insert_at(sentence, insertion_point, " " + emoji + " ")
            else:
                // Insert after specific word types for more natural placement
                words = identify_word_types(sentence)
                target_words = filter_by_type(words, position_type)
                if length(target_words) > 0:
                    word = random_choice(target_words)
                    sentence = insert_after_word(sentence, word, " " + emoji)
    
    return join_sentences(sentences)

function score_emojis(text):
    emoji_count = count_emojis(text)
    sentence_count = count_sentences(text)
    score = emoji_count / sentence_count  // ideal ratio may be tuned (e.g., 0.3 to 0.5)
    return clamp(score, 0, 1)
```

---

### 3. Punctuation Spacing Rule
**Purpose:**  
Correct punctuation spacingâ€”ensuring no spaces before punctuation and appropriate spaces after punctuationâ€”to reflect proper human typing patterns.

**Examples:**  
- **Example 1:**  
  Input: `"hello ,"` (incorrect space before comma)  
  Output: `"hello,"`
- **Example 2:**  
  Input: `"What's up?Something new"` (missing space after question mark)  
  Output: `"What's up? Something new"`
- **Example 3:**  
  Input: `"Wow!And then"` (missing space after exclamation)  
  Output: `"Wow! And then"`

**Pseudocode:**
```pseudo
function fix_punctuation_spacing(text):
    // Remove any spaces before punctuation
    text = regex_replace(text, r"\s+([,\.!?])", r"\1")
    
    // Ensure there is a space after punctuation if not already present
    text = regex_replace(text, r"([,\.!?])([^\s\d])", r"\1 \2")
    
    return text

function score_punctuation_spacing(text):
    missing_spaces = count_missing_spaces_after_punctuation(text)
    extra_spaces = count_extra_spaces_before_punctuation(text)
    total_punctuations = count_punctuation(text)
    
    if total_punctuations == 0: 
        return 0
        
    score = (missing_spaces + extra_spaces) / total_punctuations
    return clamp(score, 0, 1)
```

---

### 4. Remove Markdown Formatting Rule
**Purpose:**  
Strip out markdown syntax (e.g., bold, italics, headers, links) to present plain text that appears more natural in casual conversation.

**Examples:**  
- **Example 1:**  
  Input: `"This is **bold** text."`  
  Output: `"This is bold text."`
- **Example 2:**  
  Input: `"Visit [this link](http://example.com)"`  
  Output: `"Visit this link"`
- **Example 3:**  
  Input: `"# Heading"`  
  Output: `"Heading"`

**Pseudocode:**
```pseudo
function fix_remove_markdown(text):
    text = regex_replace(text, r"\*\*(.*?)\*\*", r"\1")  // Bold
    text = regex_replace(text, r"\*(.*?)\*", r"\1")        // Italics
    text = regex_replace(text, r"\[(.*?)\]\(.*?\)", r"\1")  // Links
    text = regex_replace(text, r"^\s*#+\s*", "", flags=MULTILINE) // Headers
    return text

function score_remove_markdown(text):
    markdown_count = count_markdown_syntax(text)
    if markdown_count == 0:
        return 0
    score = markdown_count / length(text)
    return clamp(score, 0, 1)
```

---

### 5. Sentence Length Variation Rule
**Purpose:**  
Modify sentence boundaries by merging or splitting sentences to vary length, imitating the irregular flow of human thought.

**Examples:**  
- **Example 1:**  
  Input: `"I went to the store. I bought milk."`  
  Output: `"I went to the store, and I bought milk."`
- **Example 2:**  
  Input: `"It's raining. I stayed inside."`  
  Output: `"It's raining; I stayed inside."`
- **Example 3:**  
  Input: `"I love coding. It's fun."`  
  Output: `"I love codingâ€”it's fun."`

**Pseudocode:**
```pseudo
function fix_sentence_length(text, merge_probability):
    // Don't modify if text is already short or just one sentence
    if text_is_short(text) or count_sentences(text) <= 1:
        return text
        
    sentences = split_text_into_sentences(text)
    new_sentences = []
    i = 0
    
    while i < length(sentences):
        if i < length(sentences) - 1 and random_chance(merge_probability):
            connector = random_choice([", and", ";", "â€”"])
            
            // Get the second sentence and lowercase its first letter if needed
            second_sentence = sentences[i+1]
            if second_sentence starts with uppercase:
                second_sentence = lowercase_first_letter(second_sentence)
                
            // Merge the sentences with the connector
            merged = sentences[i].rstrip(".!?") + connector + " " + second_sentence
            new_sentences.append(merged)
            i += 2
        else:
            new_sentences.append(sentences[i])
            i += 1
            
    return join_sentences(new_sentences)

function score_sentence_length(text):
    sentences = split_text_into_sentences(text)
    lengths = [word_count(s) for s in sentences]
    
    if len(sentences) <= 1:
        return 0
        
    variance = calculate_variance(lengths)
    score = min(variance / some_threshold, 1)
    return score
```

---

### 6. Fillers Injection Rule
**Purpose:**  
Inject filler words or phrases (e.g., "um", "uh", "you know") at appropriate points to simulate the natural pauses or hesitations in human speech.

**Examples:**  
- **Example 1:**  
  Input: `"I went to the store."`  
  Output: `"I, um, went to the store."`
- **Example 2:**  
  Input: `"That was amazing."`  
  Output: `"That was, you know, amazing."`
- **Example 3:**  
  Input: `"Let's start."`  
  Output: `"Let's, uh, start."`

**Pseudocode:**
```pseudo
function fix_fillers(text, filler_probability):
    fillers = ["um", "uh", "you know"]
    
    // Don't add fillers if they already exist
    if contains_any(text, fillers):
        return text
        
    sentences = split_text_into_sentences(text)
    
    for sentence in sentences:
        words = split_into_words(sentence)
        if length(words) > 3 and random_chance(filler_probability):
            filler = random_choice(fillers)
            // Insert the filler after the first word
            words.insert(1, filler)
            sentence = join_words(words)
            
    return join_sentences(sentences)

function score_fillers(text):
    filler_occurrences = count_fillers(text, ["um", "uh", "you know"])
    sentence_count = count_sentences(text)
    score = filler_occurrences / sentence_count
    return clamp(score, 0, 1)
```

---

### 7. Contraction Usage Rule
**Purpose:**  
Convert formal expressions into contractions (e.g., "do not" â†’ "don't") to create a friendlier, more casual tone.

**Examples:**  
- **Example 1:**  
  Input: `"do not"`  
  Output: `"don't"`
- **Example 2:**  
  Input: `"cannot"`  
  Output: `"can't"`
- **Example 3:**  
  Input: `"I am"`  
  Output: `"I'm"`

**Pseudocode:**
```pseudo
function fix_contractions(text):
    contraction_map = {
        "do not": "don't",
        "cannot": "can't",
        "I am": "I'm",
        "will not": "won't",
        "is not": "isn't"
    }
    
    for formal, contraction in contraction_map:
        // Use word boundary to ensure we match whole words
        text = regex_replace(text, r'\b' + formal + r'\b', contraction)
        
    return text

function score_contractions(text):
    formal_count = count_formal_expressions(text, contraction_map.keys())
    total_phrases = count_total_possible(formal_count, text)
    score = formal_count / (total_phrases + 1)
    return clamp(score, 0, 1)
```

---

### 8. Minor Typos Introduction Rule
**Purpose:**  
Occasionally introduce minor, non-disruptive typos to emulate human imperfection without sacrificing meaning.

**Examples:**  
- **Example 1:**  
  Input: `"Hello"`  
  Output: `"Helo"` (removed letter)
- **Example 2:**  
  Input: `"Friend"`  
  Output: `"Frined"` (swapped letters)
- **Example 3:**  
  Input: `"Great"`  
  Output: `"Grate"` (replaced letter)

**Pseudocode:**
```pseudo
function fix_typos(text, typo_probability):
    // Don't introduce typos if the text is already short
    if text_is_short(text):
        return text
        
    words = split_text_into_words(text)
    
    for each word in words:
        if length(word) > 3 and random_chance(typo_probability):
            typo_type = random_choice(["remove", "swap"])
            
            if typo_type == "remove" and length(word) > 4:
                // Remove one letter from the middle
                pos = random_int(1, length(word) - 2)
                word = word.substring(0, pos) + word.substring(pos + 1)
                
            else if typo_type == "swap" and length(word) > 3:
                // Swap two adjacent letters
                pos = random_int(1, length(word) - 2)
                word = word.substring(0, pos) + word[pos+1] + word[pos] + word.substring(pos + 2)
    
    return join_words(words)

function score_typos(text):
    typo_count = estimate_typos(text)
    word_count = count_words(text)
    score = typo_count / word_count
    return clamp(score, 0, 1)
```

---

### 9. Casual Language Substitution Rule
**Purpose:**  
Replace overly formal words with more common, casual alternatives to enhance a conversational vibe.

**Examples:**  
- **Example 1:**  
  Input: `"purchase"`  
  Output: `"buy"`
- **Example 2:**  
  Input: `"assist"`  
  Output: `"help"`
- **Example 3:**  
  Input: `"utilize"`  
  Output: `"use"`

**Pseudocode:**
```pseudo
function fix_casual_language(text):
    substitution_map = {
        "purchase": "buy",
        "assist": "help",
        "utilize": "use",
        "examine": "check"
    }
    
    for formal, casual in substitution_map:
        // Use word boundaries to ensure we match whole words
        text = regex_replace(text, r'\b' + formal + r'\b', casual)
        
    return text

function score_casual_language(text):
    formal_count = count_occurrences(text, list(substitution_map.keys()))
    total_words = count_words(text)
    score = formal_count / (total_words + 1)
    return clamp(score, 0, 1)
```

---

### 10. Sentence Structure Variation Rule
**Purpose:**  
Reorder clauses or restructure sentences (without changing meaning) to mimic the variety found in human expression.

**Examples:**  
- **Example 1:**  
  Input: `"I am happy because it is sunny."`  
  Output: `"Because it's sunny, I am happy."`
- **Example 2:**  
  Input: `"I like ice cream because it tastes good."`  
  Output: `"Because it tastes good, I like ice cream."`
- **Example 3:**  
  Input: `"I went home because it was late."`  
  Output: `"It was late, so I went home."`

**Pseudocode:**
```pseudo
function fix_sentence_structure(text):
    // Skip if "because" isn't in the text
    if "because" not in text:
        return text
        
    sentences = split_text_into_sentences(text)
    
    for each sentence in sentences:
        if "because" in sentence:
            parts = split_by_keyword(sentence, "because")
            
            if length(parts) == 2:
                // Get the second part and ensure it starts with a capital letter
                second_part = parts[1].strip()
                if second_part starts with lowercase:
                    second_part = capitalize_first_letter(second_part)
                    
                // Get the first part and make it lowercase
                first_part = parts[0].strip()
                
                // Rearrange: "Because [reason], [result]"
                sentence = second_part + ", so " + first_part.toLowerCase()
    
    return join_sentences(sentences)

function score_sentence_structure(text):
    structure_variants = detect_structure_variants(text)
    score = min(length(structure_variants) / ideal_variant_count, 1)
    return score
```

---

### 12. Exclamation Mark Variation Rule
**Purpose:**  
Vary the number and placement of exclamation marks to mirror human emphasis (without overdoing it).

**Examples:**  
- **Example 1:**  
  Input: `"Great."`  
  Output: `"Great!"`
- **Example 2:**  
  Input: `"Wow"`  
  Output: `"Wow!"`
- **Example 3:**  
  Input: `"Amazing"`  
  Output: `"Amazing!!"`

**Pseudocode:**
```pseudo
function fix_exclamations(text, variation_factor):
    // Don't modify if already has exclamations
    if text contains "!":
        return text
        
    sentences = split_text_into_sentences(text)
    
    for each sentence in sentences:
        // Add exclamation to sentences without ! or ?
        if "!" not in sentence and "?" not in sentence and random_chance(variation_factor):
            sentence = sentence with "." replaced by "!"
            
        // Vary number of exclamations (1-2)
        else if sentence contains "!" and random_chance(variation_factor):
            exclamation_count = weighted_random_choice([1: 0.8, 2: 0.2])
            sentence = replace_exclamations_with(sentence, "!" * exclamation_count)
    
    return join_sentences(sentences)

function score_exclamations(text):
    exclamation_count = count_character(text, "!")
    sentence_count = count_sentences(text)
    score = exclamation_count / (sentence_count + 1)
    return clamp(score, 0, 1)
```

---

### 13. Interjection Insertion Rule
**Purpose:**  
Insert interjections (e.g., "oh", "wow", "oops") at natural breakpoints to enhance spontaneity.

**Examples:**  
- **Example 1:**  
  Input: `"I forgot my keys."`  
  Output: `"Oh, I forgot my keys."`
- **Example 2:**  
  Input: `"That happened."`  
  Output: `"Wow, that happened."`
- **Example 3:**  
  Input: `"I didn't see that."`  
  Output: `"Oops, I didn't see that."`

**Pseudocode:**
```pseudo
function fix_interjections(text, interjection_probability):
    interjections = ["Oh", "Wow", "Oops"]
    
    // Don't add interjections if they already exist
    if contains_any(text, interjections):
        return text
        
    sentences = split_text_into_sentences(text)
    
    // Apply to at most one sentence
    if random_chance(interjection_probability):
        // Choose a random sentence to modify
        sentence_index = random_int(0, length(sentences) - 1)
        sentences[sentence_index] = random_choice(interjections) + ", " + sentences[sentence_index]
    
    return join_sentences(sentences)

function score_interjections(text):
    interjection_count = count_occurrences(text, interjections)
    sentence_count = count_sentences(text)
    score = interjection_count / (sentence_count + 1)
    return clamp(score, 0, 1)
```

---

### 14. Personal Pronoun Emphasis Rule
**Purpose:**  
Increase the frequency of personal pronouns and first-person references (e.g., "I", "me") to create a more personable and conversational tone.

**Examples:**  
- **Example 1:**  
  Input: `"The solution is effective."`  
  Output: `"I think the solution is effective."`
- **Example 2:**  
  Input: `"One might consider this option."`  
  Output: `"I would consider this option."`
- **Example 3:**  
  Input: `"It appears correct."`  
  Output: `"I feel it's correct."`

**Pseudocode:**
```pseudo
function fix_personal_pronouns(text):
    // Don't add if text already contains personal pronouns
    if text contains any of ["I", "me", "my", "mine"]:
        return text
        
    sentences = split_text_into_sentences(text)
    
    // Apply to at most one sentence
    if sentences:
        sentence_index = random_int(0, length(sentences) - 1)
        
        if sentences[sentence_index]:
            if sentences[sentence_index] starts with uppercase:
                sentences[sentence_index] = "I think " + sentences[sentence_index] with first letter lowercase
            else:
                sentences[sentence_index] = "I think " + sentences[sentence_index]
    
    return join_sentences(sentences)

function score_personal_pronouns(text):
    pronoun_count = count_occurrences(text, ["I", "me", "my", "mine"])
    sentence_count = count_sentences(text)
    score = pronoun_count / (sentence_count + 1)
    return clamp(score, 0, 1)
```

---

### 15. Acronym and Abbreviation Rule
**Purpose:**  
Replace longer phrases with common acronyms/abbreviations (e.g., "by the way" â†’ "btw") to simulate informal, digital communication.

**Examples:**  
- **Example 1:**  
  Input: `"by the way"`  
  Output: `"btw"`
- **Example 2:**  
  Input: `"for example"`  
  Output: `"e.g."`
- **Example 3:**  
  Input: `"that is"`  
  Output: `"i.e."`

**Pseudocode:**
```pseudo
function fix_acronyms(text):
    acronym_map = {
        "by the way": "btw",
        "for example": "e.g.",
        "that is": "i.e."
    }
    
    for phrase, acronym in acronym_map:
        // Use word boundaries to match whole phrases only
        text = regex_replace(text, r'\b' + phrase + r'\b', acronym)
        
    return text

function score_acronyms(text):
    formal_phrases_count = count_occurrences(text, acronym_map.keys())
    acronym_count = count_occurrences(text, acronym_map.values())
    score = formal_phrases_count / (formal_phrases_count + acronym_count + 1)
    return clamp(score, 0, 1)
```

---

### 16. Slang Substitution Rule
**Purpose:**  
Introduce common slang or informal expressions to replace overly formal language, thus enhancing a relaxed, human touch.

**Examples:**  
- **Example 1:**  
  Input: `"very good"`  
  Output: `"awesome"`
- **Example 2:**  
  Input: `"I am tired"`  
  Output: `"I'm beat"`
- **Example 3:**  
  Input: `"strange"`  
  Output: `"weird"`

**Pseudocode:**
```pseudo
function fix_slang(text):
    slang_map = {
        "very good": "awesome",
        "I am tired": "I'm beat",
        "strange": "weird"
    }
    
    for formal, slang in slang_map:
        // Use word boundaries to ensure we replace whole phrases
        text = regex_replace(text, r'\b' + formal + r'\b', slang)
        
    return text

function score_slang(text):
    formal_count = count_occurrences(text, slang_map.keys())
    total_words = count_words(text)
    score = formal_count / (total_words + 1)
    return clamp(score, 0, 1)
```

---

### 17. Sentence Fragmentation Rule
**Purpose:**  
Introduce sentence fragments or ellipses to simulate natural, spoken language and spontaneous thought.

**Examples:**  
- **Example 1:**  
  Input: `"I was thinking about what you said."`  
  Output: `"I was thinking... about what you said."`
- **Example 2:**  
  Input: `"Went to the store. Got milk."`  
  Output: `"Went to the store, got milk."`
- **Example 3:**  
  Input: `"Really enjoyed it. Very much."`  
  Output: `"Really enjoyed it, very much."`

**Pseudocode:**
```pseudo
function fix_fragmentation(text, fragmentation_probability):
    // Don't apply to short text
    if length(text) < 40:
        return text
        
    // Don't apply if already has ellipsis
    if text contains "...":
        return text
        
    sentences = split_text_into_sentences(text)
    
    // Apply to at most one part of the text
    fragmented = false
    result = []
    
    for i = 0 to length(sentences) - 1:
        if not fragmented and i < length(sentences) - 1 and both sentences are short and random_chance(fragmentation_probability):
            // Lowercase the first letter of the second sentence if needed
            second_sentence = sentences[i+1]
            if second_sentence starts with uppercase:
                second_sentence = lowercase_first_letter(second_sentence)
                
            // Choose a connector and combine sentences
            connector = random_choice([", ", "... ", " - ", "; "])
            combined = sentences[i] without trailing punctuation + connector + second_sentence
            
            result.append(combined)
            i += 2
            fragmented = true
            
        else if not fragmented and sentences[i] is long and random_chance(fragmentation_probability):
            // Break a sentence at a comma
            parts = split_by_comma(sentences[i])
            
            if length(parts) > 1:
                break_point = random_int(1, length(parts) - 1)
                first_part = join_parts(parts.slice(0, break_point)) without trailing punctuation
                second_part = join_parts(parts.slice(break_point))
                
                result.append(first_part + "...")
                result.append(second_part)
                fragmented = true
            else:
                result.append(sentences[i])
                
        else:
            result.append(sentences[i])
    
    return join_sentences(result)

function score_fragmentation(text):
    ellipsis_count = count_occurrences(text, "...")
    sentence_count = count_sentences(text)
    score = ellipsis_count / (sentence_count + 1)
    return clamp(score, 0, 1)
```

---

### 18. Verbosity Reduction Rule
**Purpose:**  
Simplify overly verbose or formal language to a more succinct, conversational form.

**Examples:**  
- **Example 1:**  
  Input: `"It is my opinion that"`  
  Output: `"I think"`
- **Example 2:**  
  Input: `"Due to the fact that"`  
  Output: `"Because"`
- **Example 3:**  
  Input: `"In the event that"`  
  Output: `"If"`

**Pseudocode:**
```pseudo
function fix_verbosity(text):
    verbosity_map = {
        "It is my opinion that": "I think",
        "Due to the fact that": "Because",
        "In the event that": "If"
    }
    
    for long_form, short_form in verbosity_map:
        // Use word boundaries and escaping to match exact phrases
        text = regex_replace(text, r'\b' + escape(long_form) + r'\b', short_form)
        
    return text

function score_verbosity(text):
    long_form_count = count_occurrences(text, verbosity_map.keys())
    total_words = count_words(text)
    score = long_form_count / (total_words + 1)
    return clamp(score, 0, 1)
```

---

### 19. Excessive AI Mention Removal Rule
**Purpose:**  
Strip out phrases that explicitly reference the model's AI nature (e.g., "as an AI") to avoid breaking the illusion of human conversation.

**Examples:**  
- **Example 1:**  
  Input: `"As an AI, I think"`  
  Output: `"I think"`
- **Example 2:**  
  Input: `"As an AI, I cannot"`  
  Output: `"I cannot"`
- **Example 3:**  
  Input: `"I'm an AI."`  
  Output: `""`

**Pseudocode:**
```pseudo
function fix_ai_mentions(text):
    ai_phrases = ["As an AI,", "I'm an AI", "As an artificial intelligence"]
    
    for phrase in ai_phrases:
        text = text.replace(phrase, "")
        
    return text

function score_ai_mentions(text):
    ai_count = count_occurrences(text, ai_phrases)
    total_sentences = count_sentences(text)
    score = ai_count / (total_sentences + 1)
    return clamp(score, 0, 1)
```

---

### 20. Tone Warmth Adjustment Rule
**Purpose:**  
Modify the overall tone to be warmer and more empathetic by adding friendly phrases and softening direct language.

**Examples:**  
- **Example 1:**  
  Input: `"The solution is provided."`  
  Output: `"I hope, the solution is provided."`
- **Example 2:**  
  Input: `"The error occurred."`  
  Output: `"Oops, the error occurred."`
- **Example 3:**  
  Input: `"Invalid input."`  
  Output: `"Hmm, invalid input."`

**Pseudocode:**
```pseudo
function fix_tone_warmth(text, warmth_level):
    friendly_prefixes = ["I hope", "Oops", "Hmm"]
    
    // Don't add friendly prefixes if they already exist
    if contains_any(text, friendly_prefixes):
        return text
        
    sentences = split_text_into_sentences(text)
    
    // Apply to at most one sentence
    if random_chance(warmth_level):
        // Choose a random sentence to modify
        sentence_index = random_int(0, length(sentences) - 1)
        
        if sentences[sentence_index]:
            sentences[sentence_index] = random_choice(friendly_prefixes) + ", " + sentences[sentence_index]
    
    return join_sentences(sentences)

function score_tone_warmth(text):
    friendly_count = count_occurrences(text, friendly_prefixes)
    sentence_count = count_sentences(text)
    score = friendly_count / (sentence_count + 1)
    return clamp(score, 0, 1)
```

---

Each of these rules works as a "fix" function to modify the LLM output toward a more human-like style and includes a corresponding "score" function (returning a value between 0 and 1) to measure how strongly that dimension appears. By combining these deterministic rules, one can achieve a composite humanization effect while preserving the original semantics of the text.